{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67742338",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\VirtualEnvs\\p_ds\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/samuelcortinhas/cats-and-dogs-image-classification?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64.4M/64.4M [00:19<00:00, 3.50MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\devar\\.cache\\kagglehub\\datasets\\samuelcortinhas\\cats-and-dogs-image-classification\\versions\\4\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "from tensorflow.keras.layers import Dense,Softmax\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# !pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"samuelcortinhas/cats-and-dogs-image-classification\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4862317c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 557 files belonging to 2 classes.\n",
      "Found 140 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_height, img_width = 32, 32  # Resize all images to same size\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    f'{path}/train',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'   # gives integer labels\n",
    ")\n",
    "\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    f'{path}/test',\n",
    "    image_size=(img_height, img_width),\n",
    "    batch_size=batch_size,\n",
    "    label_mode='int'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d61ef0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "p:\\VirtualEnvs\\p_ds\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:92: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 61ms/step - accuracy: 0.5063 - loss: 2.7128 - val_accuracy: 0.5000 - val_loss: 0.7033\n",
      "Epoch 2/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.4901 - loss: 0.8233 - val_accuracy: 0.5000 - val_loss: 0.7335\n",
      "Epoch 3/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.5314 - loss: 0.7102 - val_accuracy: 0.4857 - val_loss: 0.7067\n",
      "Epoch 4/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.5566 - loss: 0.6788 - val_accuracy: 0.4643 - val_loss: 0.6963\n",
      "Epoch 5/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.5871 - loss: 0.6613 - val_accuracy: 0.5000 - val_loss: 0.7456\n",
      "Epoch 6/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5943 - loss: 0.6749 - val_accuracy: 0.5143 - val_loss: 0.7516\n",
      "Epoch 7/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.5978 - loss: 0.6539 - val_accuracy: 0.5071 - val_loss: 0.7641\n",
      "Epoch 8/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.6140 - loss: 0.6471 - val_accuracy: 0.5143 - val_loss: 0.7265\n",
      "Epoch 9/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step - accuracy: 0.6266 - loss: 0.6419 - val_accuracy: 0.5071 - val_loss: 0.7465\n",
      "Epoch 10/10\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 55ms/step - accuracy: 0.5961 - loss: 0.6704 - val_accuracy: 0.5429 - val_loss: 0.7370\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - accuracy: 0.5429 - loss: 0.7370\n",
      "Test accuracy: 0.54\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step \n",
      "Predicted: cat, True: cat\n",
      "Predicted: dog, True: dog\n",
      "Predicted: cat, True: dog\n",
      "Predicted: cat, True: dog\n",
      "Predicted: cat, True: dog\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3072 into shape (128,128,3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m5\u001b[39m):\n\u001b[32m     44\u001b[39m     plt.subplot(\u001b[32m1\u001b[39m, \u001b[32m5\u001b[39m, i+\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     plt.imshow(\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     46\u001b[39m     pred_label = class_names[tf.argmax(predictions[i])]\n\u001b[32m     47\u001b[39m     true_label = class_names[labels[i]]\n",
      "\u001b[31mValueError\u001b[39m: cannot reshape array of size 3072 into shape (128,128,3)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flatten_and_normalize(images, labels):\n",
    "    images = tf.reshape(images, (tf.shape(images)[0], -1)) / 255.0\n",
    "    return images, labels\n",
    "\n",
    "train_dataset = train_dataset.map(flatten_and_normalize)\n",
    "test_dataset = test_dataset.map(flatten_and_normalize)\n",
    "\n",
    "class_names = [\"cat\",\"dog\"]  # Put your class names here\n",
    "num_classes = len(class_names)\n",
    "\n",
    "\n",
    "input_shape = img_height * img_width * 3  # 3 channels for RGB\n",
    "model = Sequential([\n",
    "    Dense(512, activation='relu', input_shape=(input_shape,)),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(num_classes)\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "model.fit(train_dataset, epochs=10, validation_data=test_dataset)\n",
    "\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.2f}\")\n",
    "\n",
    "\n",
    "probability_model = Sequential([model, Softmax()])\n",
    "\n",
    "for images, labels in test_dataset.take(1):\n",
    "    predictions = probability_model.predict(images)\n",
    "    for i in range(5):\n",
    "        print(f\"Predicted: {class_names[tf.argmax(predictions[i])]}, True: {class_names[labels[i]]}\")\n",
    "\n",
    "\n",
    "for images, labels in test_dataset.take(1):\n",
    "    predictions = probability_model.predict(images)\n",
    "    plt.figure(figsize=(12, 3))\n",
    "    for i in range(5):\n",
    "        plt.subplot(1, 5, i+1)\n",
    "        plt.imshow(images[i].numpy().reshape(128, 128, 3))\n",
    "        pred_label = class_names[tf.argmax(predictions[i])]\n",
    "        true_label = class_names[labels[i]]\n",
    "        color = 'green' if pred_label == true_label else 'red'\n",
    "        plt.title(pred_label, color=color)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6cb7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P:\\VirtualEnvs\\p_ds (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
